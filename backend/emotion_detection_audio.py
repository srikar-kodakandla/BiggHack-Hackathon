    """
    This file contains the code for emotion detection from audio.
    The emotion detection is done using the pretrained model from speechbrain.
    The model is trained on the IEMOCAP dataset.
    The model is a wav2vec2 model with a classifier on top of it.
    The model is trained on the 4 emotions: happy, sad, angry and neutral.
    The output of the model is the probability of each emotion.
    
    Functions:
    1. process_video(video_path, chunk_duration=5):
        - Processes a video file by extracting audio chunks and performing sentiment analysis on each chunk.
        - Parameters:
            - video_path (str): Path to the video file.
            - chunk_duration (float, optional): Duration (in seconds) of each audio chunk. Default is 5 seconds.
        - Returns:
            - output_dict (dict): Dictionary containing the processed data.
                - 'final_emotion' (dict): Dictionary of emotions for each chunk, where the keys are timestamps.
                - 'complete_emotion' (list): List of emotions for the entire video.
                - 'emotion_for_chunks' (dict): Dictionary of emotions for each chunk, where the keys are timestamps.
    
    2. perform_sentiment_analysis(audio_path):
        - Performs sentiment analysis on an audio file using the SpeechBrain library.
        - Parameters:
            - audio_path (str): Path to the audio file.
        - Returns:
            - text_lab (str): Emotion label generated by the sentiment analysis.
    
    3. get_video_length(video_path):
        - Retrieves the duration of a video file.
        - Parameters:
            - video_path (str): Path to the video file.
        - Returns:
            - duration (float): Duration of the video in seconds.

    4. main_video(video_path):
        - Main function to process a video and obtain the sentiment analysis results.
        - Parameters:
            - video_path (str): Path to the video file.
        - Returns:
            - result_dict (dict): Dictionary containing the processed data from process_video function.

Note:
    - The script relies on the availability of the speechbrain/emotion-recognition-wav2vec2-IEMOCAP pretrained model
      for sentiment analysis. Make sure the model is properly installed and accessible.

Example usage:
    video_path = "demo.mp4"
    result = main_video(video_path)
    print(result)
    """

from moviepy.editor import VideoFileClip
import numpy as np
import math
import cv2

def process_video(video_path, chunk_duration=5):
    video_clip = VideoFileClip(video_path)
    audio_clip = video_clip.audio
    audio_duration = audio_clip.duration

    num_chunks = math.ceil(audio_duration / chunk_duration)

    output_dict = {'final_emotion': {}, 'complete_emotion': None}
    output_dict['emotion_for_chunks'] = {} 

    for i in range(num_chunks):
        start_time = i * chunk_duration
        end_time = min((i + 1) * chunk_duration, audio_duration)

        output_path = f"output_audio_chunk_{i}.wav"

        audio_chunk = audio_clip.subclip(start_time, end_time)
        audio_chunk.write_audiofile(output_path)

        text_lab = perform_sentiment_analysis(output_path)
        output_dict['emotion_for_chunks'][i+1] = text_lab

    complete_text_lab = perform_sentiment_analysis(video_path)
    output_dict['complete_emotion'] = complete_text_lab
    
    keys = list(np.arange(0, chunk_duration * 10, chunk_duration))
    
    values = list(output_dict['emotion_for_chunks'].values())
    if len(keys) == len(values):
        output_dict['emotion_for_chunks'] = dict(zip(keys, values))
    emotion_dict = {
    "neu": "Neutral",
    "N": "Neutral",
    "hap": "Happy",
    "H": "Happy",
    "sad": "Sad",
    "S": "Sad",
    "ang": "Angry",
    "A": "Angry",
    "fea": "Fearful",
    "F": "Fearful",
    "sur": "Surprise",
    "SU": "Surprise",
    "dis": "Disgust",
    "D": "Disgust"
    }

    output_dict['complete_emotion'] = [emotion_dict[emotion] for emotion in output_dict['complete_emotion']]
    output_dict['emotion_for_chunks'] = {key: [emotion_dict[emotion] for emotion in emotions] for key, emotions in output_dict['emotion_for_chunks'].items()}



    return output_dict

def perform_sentiment_analysis(audio_path):
    from speechbrain.pretrained.interfaces import foreign_class
    classifier = foreign_class(source="speechbrain/emotion-recognition-wav2vec2-IEMOCAP", pymodule_file="custom_interface.py", classname="CustomEncoderWav2vec2Classifier")
    out_prob, score, index, text_lab = classifier.classify_file(audio_path)
    return text_lab

video_path = "demo.mp4"

def get_video_length(video_path):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("Error opening video file")
        return None

    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = frame_count / fps

    cap.release()

    return duration

def main_video(video_path):

    split_time=get_video_length(video_path)/9

    result_dict = process_video(video_path, chunk_duration=get_video_length(video_path)/9)
    return result_dict

